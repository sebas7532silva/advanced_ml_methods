{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "## Nombres: Freddy Silva, César Cruz, Fernando Guevara, Kailin Wu\n",
    "## Matrículas: A00828792, A00825747, A00828723, A00830574\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/media/pepe/DataUbuntu/Databases/asl_data/'\n",
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Create a function that will allow you to split the previously loaded validation set\n",
    "    into valition and test.\n",
    "\n",
    "    Parameters:\n",
    "        - x: Array of feature data.\n",
    "        - y: Array of target labels corresponding to x.\n",
    "        - pct: Proportion of data to allocate to the test set.\n",
    "        - shuffle: Whether to randomly shuffle the data before spitting.\n",
    "\n",
    "    Returns:\n",
    "        - x_val, y_val: Validation features and labels\n",
    "        - x_test, y_test: Test features and labels\n",
    "    '''\n",
    "\n",
    "    # Total number of samples in the dataset\n",
    "    n = len(x)\n",
    "\n",
    "    # Create an array of indices from 0 to n-1\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # Shuffle indices randomly if shuffle=True, this ensures the split is random.\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    # Determine the split point based on the percentage\n",
    "    split = int(n * (1 - pct))\n",
    "\n",
    "    # Indices for validation and test set\n",
    "    val_idx = idx[:split]\n",
    "    test_idx = idx[split:]\n",
    "\n",
    "    # Select validation and test data using the indices\n",
    "    x_val, y_val = x[val_idx], y[val_idx]\n",
    "    x_test, y_test = x[test_idx], y[test_idx]\n",
    "\n",
    "    # Return validation and test splits\n",
    "    return x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to split data into validation and test\n",
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "### The following\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise\n",
    "\n",
    "This section performs feature normalization on the dataset. Specifically, it applies Z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x_mean, x_std, x_data):\n",
    "    \"\"\"\n",
    "    Standardizes input data using Z-score normalization.\n",
    "\n",
    "    Parameters:\n",
    "        - x_mean: Mean value computed from the training data.\n",
    "        - x_std: Standard deviation value computed from the training data.\n",
    "        - x_data: Data to be normalized\n",
    "    \n",
    "    Returns:\n",
    "        - Normalized version of x_data.\n",
    "    \"\"\"\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the training data.\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "# Normalize the training data using its own mean and std\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "\n",
    "# Normalize the validation and test data using the training mean and std\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6268384e-06, 0.99999946)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the training data is now standardized\n",
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples\n",
    "\n",
    "This section is used to visually inspect a random sample from the test dataset to ensure that the images and their lables are correctly aligned after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(image):\n",
    "    \"\"\"\n",
    "    Displays a grayscale image of a sign language.\n",
    "\n",
    "    Parameters:\n",
    "        - image: Falttened image array\n",
    "    \n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(image.reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  6, 19, ...,  7,  0, 12])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contains the true labels of the test dataset.\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sampled image represents a: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly select an index from the test set.\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "\n",
    "# Print the true lable of the selected image\n",
    "print(f'The sampled image represents a: {y_test[rnd_idx]}')\n",
    "\n",
    "# Display the corresponding image\n",
    "plot_number(x_test[rnd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations for our model\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches\n",
    "\n",
    "This function is responsible for splitting the dataset into mini-batches, which are small subsets of the data used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    \"\"\"\n",
    "    Create mini-batches from the input dataset.\n",
    "\n",
    "    Parameters:\n",
    "        - mb_size: Size of each mini-batch.\n",
    "        - x: Feature data.\n",
    "        - y: Label data.\n",
    "        - shuffle: Whether to randomly shuffle the data before creating mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "        - A generator that yields tuples (x_batch, y_batch),\n",
    "        where each tuple corresponds to one mini-batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the number of samples in x and y is the same.\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "\n",
    "    # Total number of samples.\n",
    "    total_data = x.shape[0]\n",
    "\n",
    "    # Shuffle the dataset if requested.\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "\n",
    "    # Generate mini-batches\n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is serves as a semantic alias for numpy arrays\n",
    "# that are intended to be used as tensors within a ML context.\n",
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear class\n",
    "\n",
    "This class represents a fully connected neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    \"\"\"\n",
    "    Fully connected layer.\n",
    "\n",
    "    This class implements a linear transformation of the form:\n",
    "        Z = W @ X + b\n",
    "    \n",
    "    where W and b are trainable parameters initialized using \n",
    "    Kaiming He initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the linear layer.\n",
    "\n",
    "        Parameters:\n",
    "            - input_size: Number of input features.\n",
    "            - output_size: Number of output features.\n",
    "        \"\"\"\n",
    "\n",
    "        # Weight matrix of shape\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "\n",
    "        # Bias vector of shape\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "\n",
    "    def __call__(self, X): \n",
    "        \"\"\"\n",
    "        Forward pass of the linear layer.\n",
    "\n",
    "        Computes the linear transformation:\n",
    "            Z = W @ X + b\n",
    "\n",
    "        Parameters:\n",
    "            - X: Input tensor of shape.\n",
    "\n",
    "        Returns\n",
    "            - Output tensor Z of shape.\n",
    "        \"\"\"\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Z):\n",
    "        \"\"\"\n",
    "        Backward pass of the linear layer.\n",
    "\n",
    "        Computes gradients of the loss with respect to the input X, the weights W,\n",
    "        and the bias b.\n",
    "\n",
    "        Parameters:\n",
    "            - X: Input tensor used in the forward pass.\n",
    "            - Z: Output tensor from the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Gradient with respect to the input\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "\n",
    "        # Gradient with respect to the weights\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "\n",
    "        # Gradient with respect to the bias\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU\n",
    "This class implements the ReLU activation function, which is one of the most widely used activation functions in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"\n",
    "\n",
    "    This class implements the ReLu activation:\n",
    "        ReLU(Z) = max(0, Z)\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, Z):\n",
    "        \"\"\"\n",
    "        Foward pass of the ReLU activation. \n",
    "        \n",
    "        Parameters:\n",
    "            - Z: Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            - Output tensor after applying ReLU.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def backward(self, Z, A):\n",
    "        \"\"\"\n",
    "        Backward pass of the ReLU activation.\n",
    "\n",
    "        Parameters:\n",
    "            - Z: Input tensor to the ReLU during the forward pass.\n",
    "            - A: Output tensor from the ReLU.\n",
    "        \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy gradient from the next layer.\n",
    "        Z.grad = A.grad.copy()\n",
    "    \n",
    "        # Zero out gradients where Z was inactive.\n",
    "        Z.grad[Z <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential\n",
    "\n",
    "This class works like a minimal version of Keras/PyTorch Sequential:\n",
    "* You pass a list of layers: Lineas, ReLU, Linear, etc.\n",
    "* It runs them in order during the forward pass.\n",
    "* It stores intermediate outputs so it can do backpropagation.\n",
    "* It updates parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    \"\"\"\n",
    "    A simple sequential container for neural network layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Initializes the sequential model.\n",
    "\n",
    "        Parameters:\n",
    "            - Layers: List of layer objects to be applied in sequence.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "\n",
    "        Parameters:\n",
    "            - X: Input tensor of shape.\n",
    "\n",
    "        Returns:\n",
    "            - Output tensor after applying all layers.\n",
    "        \"\"\"\n",
    "        self.x = X \n",
    "\n",
    "        # Store the input as the \"output\" of layer 0 \n",
    "        self.outputs['l0'] = self.x\n",
    "\n",
    "        # Apply each layer sequentially and store outputs\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs['l'+str(i)]=self.x\n",
    "\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass through all layers.\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate layers backwards\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "\n",
    "            # For layer i, input is output of l{i}, output is outputs of l{i+1}\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
    "\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        \"\"\"\n",
    "        Updates trainable parameters using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "            - learning_rate: Step size for gradient descent updates.\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # Skip activation layers\n",
    "            if isinstance(layer, ReLU): continue\n",
    "\n",
    "            # Gradient descent update for Linear-like layers\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generates predictions from the model output.\n",
    "\n",
    "        Parameters:\n",
    "            - X: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            - Predicted class index based on argmax.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.__call__(X))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "This section implements the Softmax + Cross-Entropy cost function, which is the standard loss function used for multi-class classification in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    \"\"\"\n",
    "    Computes Softmax probabilities + Cross-Entropy loss, and sets the gradient dL/dx.\n",
    "\n",
    "    Paramaters:\n",
    "        - x: Raw class scores.\n",
    "        - y: True class labels.\n",
    "    \n",
    "    Returns:\n",
    "        - preds: Softmax probabilities.\n",
    "        - cost: Mean cross-entropy loss over the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of samples in the batch\n",
    "    batch_size = x.shape[1]\n",
    "\n",
    "    # Convert logits to exponentialed scores\n",
    "    exp_scores = np.exp(x)\n",
    "\n",
    "    # Softmax probabilities per sample\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "    \n",
    "    # Keep a copy of probabilities for output\n",
    "    preds = probs.copy()\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    # Extract the probability assigned to the correct class for each sample\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "\n",
    "    # Compute mean negative log-likelihood\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    \n",
    "    # Gradient computation\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 #dl/dx\n",
    "    x.grad = probs.copy()\n",
    "    \n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "This section implements the complete training and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate = 1e-3):\n",
    "    \"\"\"\n",
    "    Trains a neural network model using mini-batch gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        - model: Neural network model to be trained.\n",
    "        - epochs: Number of full passes over the training dataset.\n",
    "        - mb_size: Mini-batch size used during training.\n",
    "        - learning_rate: Learning rate for gradient descent updates.\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Iterates over training data in mini-batches\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "\n",
    "            # Forward pass: compute class scores\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "\n",
    "            # Compute loss and initialize gradient for backpropagation\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "\n",
    "            # Backward pass: propagate gradients through all layers\n",
    "            model.backward()\n",
    "\n",
    "            # Update model parameters using gradient descent\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        # Evaluate and report validation accuracy after each epoch\n",
    "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    \"\"\"\n",
    "    Computes classification accuracy of the model.\n",
    "\n",
    "    Parameters:\n",
    "        - x: Input feature matrix.\n",
    "        - y: True class labels.\n",
    "        - mb_size: Mini-batch size used during evaluation.\n",
    "\n",
    "    Returns:\n",
    "        - Classification accuracy as a value between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Counter for correctly classified samples\n",
    "    correct = 0\n",
    "\n",
    "    # Counter for total evaluated samples\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over the dataset in mini-batches\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "\n",
    "        # Forward pass to obtain predictions\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "\n",
    "        # Update total number of samples\n",
    "        total += pred.shape[1]\n",
    "    \n",
    "    # Return accuracy ratio\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it\n",
    "\n",
    "This section builds, trains, and evaluates a neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential neural network model\n",
    "model = Sequential_layers([Linear(784, 200), ReLU(), Linear(200, 200), ReLU(), Linear(200, 24)])\n",
    "\n",
    "# Define training hyperparameters\n",
    "mb_size = 512\n",
    "learning_rate = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 1.080674878070714, accuracy: 0.5635805911879531\n",
      "costo: 0.5515472250600049, accuracy: 0.6514221974344674\n",
      "costo: 0.3367789802699019, accuracy: 0.7027328499721138\n",
      "costo: 0.2143201762076599, accuracy: 0.7189068600111544\n",
      "costo: 0.16977569152924987, accuracy: 0.7239263803680982\n",
      "costo: 0.11317767145043749, accuracy: 0.7350808700501952\n",
      "costo: 0.09841417740494689, accuracy: 0.7328499721137758\n",
      "costo: 0.06470988307708993, accuracy: 0.7412158393753486\n",
      "costo: 0.04802686277814786, accuracy: 0.7428890128276632\n",
      "costo: 0.04563404840841216, accuracy: 0.7459564974902398\n",
      "costo: 0.03605386656863122, accuracy: 0.7484662576687117\n",
      "costo: 0.03112311981566811, accuracy: 0.7518126045733408\n",
      "costo: 0.03556637768965613, accuracy: 0.7520914668153932\n",
      "costo: 0.025642243660553392, accuracy: 0.7484662576687117\n",
      "costo: 0.025697094056787884, accuracy: 0.7537646402677077\n",
      "costo: 0.021713999223310163, accuracy: 0.7573898494143892\n",
      "costo: 0.021528240609258877, accuracy: 0.7593418851087562\n",
      "costo: 0.019459038127382162, accuracy: 0.7557166759620747\n",
      "costo: 0.01826340770145685, accuracy: 0.7593418851087562\n",
      "costo: 0.014870050748588222, accuracy: 0.7593418851087562\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the defined training loop\n",
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7718906860011154\n"
     ]
    }
   ],
   "source": [
    "# Evaluate final model performance on the test dataset\n",
    "print(accuracy(x_test, y_test, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set\n",
    "\n",
    "This section performs a qualitative evaluation of the trained model by testing it on a randomly selected sample from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value is: y the true value is:y\n"
     ]
    }
   ],
   "source": [
    "# Select a random index from the test set\n",
    "idx = np.random.randint(len(y_test))\n",
    "\n",
    "# Plot the corresponding test image\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "\n",
    "# Predict the class for the selected test sample\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "\n",
    "# Print the predicted label and true label\n",
    "print(f'The predicted value is: {alphabet[pred]} the true value is:{alphabet[y_test[idx]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
